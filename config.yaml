llmx:
  typex: "local" # "local"
  modelx: "Open-Orca/oo-phi-1_5" # "google/flan-t5-xxl" "meta-llama/Llama-2-70b-chat" "Open-Orca/oo-phi-1_5"  "microsoft/phi-1_5" microsoft/phi-2, 'mistralai/Mistral-7B-Instruct-v0.2'
llm:
  type: "api" # "local"
  model: "google/flan-t5-xxl"
chunking:
  chunk_size: 1000
  chunk_overlap: 100
search: 
  search_strategy: hybrid # dense, hybrid
  alpha: 0.5 # 1 pure vector search, 0 pure keyword search
k_top_chunks: 3
rerank: False
memory:
  type: ConversationBufferWindowMemory # ConversationBufferMemory Entity ConversationSummaryMemory
  window: 5